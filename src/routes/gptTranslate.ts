import { Router } from "express";
import { OpenAI } from "openai";
import dotenv from "dotenv";

dotenv.config();

const router = Router();
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

router.get("/", async (req, res) => {
  res.send("gptRoutes");
});

router.post("/translateToEnglish", async (req, res) => {
  try {
    const chineseSpeech = req.body.chineseSpeech;

    if (!chineseSpeech) {
      res.status(400).json({ error: "è«‹æä¾›ä¸­æ–‡æ¼”è¬›ç¨¿" });
      return;
    }
    
    const prompt = "è«‹å°‡ä»¥ä¸‹ä¸­æ–‡æ¼”è¬›ç¨¿ç¿»è­¯æˆè‹±æ–‡ï¼Œé¢¨æ ¼å£èªåŒ–ã€è‡ªç„¶æµæš¢ï¼Œé©åˆåœ¨å ±å‘Šä¸­å£èªªä½¿ç”¨ï¼Œä¸è¦éæ–¼æ›¸é¢åŒ–ã€‚" +
                   "å¯ä»¥æ ¹æ“šè‹±æ–‡èªæ„Ÿç•¥ä½œèª¿æ•´ï¼Œä½¿å…§å®¹åœ¨è‹±æ–‡ä¸­è½èµ·ä¾†æ›´é †æš¢ã€‚æ•´é«”é•·åº¦æ§åˆ¶åœ¨ 90 ç§’å…§ã€‚" +
                   "è«‹æ ¹æ“šèªæ„çµæ§‹é€²è¡Œåˆç†åˆ†æ®µï¼Œæ¯ä¸€æ®µè¦æœ‰å››å¥ä»¥ä¸Šï¼Œé©ç•¶ä½¿ç”¨æ›è¡Œåˆ†éš”æ®µè½ï¼Œé¿å…å¥å­ä¸­æ–·æˆ–éçŸ­æ®µè½ã€‚" +
                   "ä¸éœ€è¦åŠ ä¸Šèªªæ˜æˆ–çµå°¾ï¼Œä¸è¦æ“´å±•å¥å­ï¼Œåªè¼¸å‡ºç¿»è­¯å¾Œçš„è‹±æ–‡è¬›ç¨¿å³å¯ï¼Œä¸¦ä¸”ä¸€å€‹æ®µè½å°±ä»¥å…©å€‹æ›è¡Œåˆ†é–‹ã€‚" +
                   "ä»¥ä¸‹æ˜¯ä¸­æ–‡ç¨¿ï¼šã€Œ" + chineseSpeech + "ã€";

    const response = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      messages: [
        { role: "system", content: "æˆ‘æ˜¯ä¸€ä½ç ”ç©¶è€…ï¼Œç›®æ¨™æ˜¯è®“è¬›ç¨¿å…§å®¹è‡ªç„¶æµæš¢ã€å°ˆæ¥­æ¸…æ™°ï¼Œé©åˆåœ¨ç°¡å ±ä¸­å£èªè¡¨é”ã€‚" },
        { role: "user", content: prompt }
      ],
      temperature: 0.7,
    });
    console.log("ğŸ”„ GPT API å›æ‡‰:", response.choices[0]?.message?.content);
    res.json({
      reply : response.choices[0]?.message?.content
    });
    return;
  } catch (error) {
    console.error("âŒ GPT API è«‹æ±‚éŒ¯èª¤:", error);
    res.status(500).json({ error: "GPT API ç™¼ç”ŸéŒ¯èª¤" });
  }
});

// not prefer to use
router.post("/noSpeechWithImg", async (req, res) => {
  try {
    const { model, userinfo, prompt, imageUrl, temperature } = req.body;

    if (!model && !prompt && ! userinfo && !temperature && !imageUrl) {
      res.status(400).json({ error: "è«‹æä¾›åƒæ•¸" });
      return;
    }

    const response = await openai.chat.completions.create({
      model: model,
      messages: [
        { role: "system", content: userinfo },
        { role: "user", content: [
          { type: "text", text: prompt },
          { type: "image_url", image_url: { url: imageUrl },},
        ],
        },
      ],
      temperature: temperature || 0.5,
    });
    res.json({
      model : model,
      userinfo : userinfo,
      prompt : prompt,
      temperature : temperature,
      reply : response.choices[0]?.message?.content
    });
    return;
  } catch (error) {
    console.error("âŒ GPT API è«‹æ±‚éŒ¯èª¤:", error);
    res.status(500).json({ error: "GPT API ç™¼ç”ŸéŒ¯èª¤" });
  }
});

router.post("/WithSpeechNoImg", async (req, res) => {
  try {
    const { model, userinfo, prompt, temperature } = req.body;

    if (!model && !prompt && ! userinfo && !temperature) {
      res.status(400).json({ error: "è«‹æä¾›åƒæ•¸" });
      return;
    }

    const response = await openai.chat.completions.create({
      model: model,
      messages: [
        { role: "system", content: userinfo },
        { role: "user", content: prompt }
      ],
      temperature: temperature || 0.5,
    });
    res.json({
      model : model,
      userinfo : userinfo,
      prompt : prompt,
      temperature : temperature,
      reply : response.choices[0]?.message?.content
    });
    return;
  } catch (error) {
    console.error("âŒ GPT API è«‹æ±‚éŒ¯èª¤:", error);
    res.status(500).json({ error: "GPT API ç™¼ç”ŸéŒ¯èª¤" });
  }
});

export default router;